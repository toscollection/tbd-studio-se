<%@ jet 
imports="
		java.util.List
		java.util.Map
		org.talend.core.model.process.ElementParameterParser	
		org.talend.core.model.process.INode
		org.talend.core.model.metadata.IMetadataTable 
		org.talend.core.model.metadata.IMetadataColumn 
		org.talend.designer.codegen.config.CodeGeneratorArgument
		org.talend.core.model.process.IConnection
		org.talend.core.model.process.IConnectionCategory
		org.talend.core.model.metadata.types.JavaTypesManager
		org.talend.core.model.metadata.types.JavaType
		"
%>

	<%@ include file="@{org.talend.designer.components.localprovider}/components/templates/Log4j/Log4jFileUtil.javajet"%>
	
<%
	CodeGeneratorArgument codeGenArgument = (CodeGeneratorArgument) argument;
	INode node = (INode)codeGenArgument.getArgument();
	String cid = node.getUniqueName();
	String useExistingConn = ElementParameterParser.getValue(node,"__USE_EXISTING_CONNECTION__");
	String zookeeper_quorum = ElementParameterParser.getValue(node, "__ZOOKEEPER_QUORUM__");
	String zookeeper_client_port = ElementParameterParser.getValue(node, "__ZOOKEEPER_CLIENT_PORT__");
	String table_name = ElementParameterParser.getValue(node, "__TABLE__");
	boolean isByFilter = ("true").equals(ElementParameterParser.getValue(node, "__IS_BY_FILTER__"));
	boolean defineSelection = "true".equals(ElementParameterParser.getValue(node, "__DEFINE_ROW_SELECTION__"));
	
	log4jFileUtil.componentStartInfo(node);

	java.util.List<String> supportKrbVersionList = java.util.Arrays.<String>asList(
	"Cloudera_CDH4","Cloudera_CDH4_YARN","Cloudera_CDH5","Cloudera_CDH5_1","Cloudera_CDH5_1_MR1",
	"HDP_1_0","HDP_1_2","HDP_1_3","HDP_2_0","HDP_2_1","HDP_2_2",
	"APACHE_1_0_0","APACHE_1_0_3_EMR",
	"PIVOTAL_HD_2_0");
%>
int nb_line_<%=cid%> = 0;
org.apache.hadoop.conf.Configuration conn_<%=cid %>=null;
<% 
// not use existing connection
if(!"true".equals(useExistingConn)){

	String hbaseVersion = ElementParameterParser.getValue(node, "__HBASE_VERSION__");
	boolean useKrb = "true".equals(ElementParameterParser.getValue(node, "__USE_KRB__"));
	boolean useKeytab = "true".equals(ElementParameterParser.getValue(node, "__USE_KEYTAB__"));
	String userPrincipal = ElementParameterParser.getValue(node, "__PRINCIPAL__");
	String keytabPath = ElementParameterParser.getValue(node, "__KEYTAB_PATH__");
	String hbaseMasterPrincipal = ElementParameterParser.getValue(node, "__HBASE_MASTER_PRINCIPAL__");
	String hbaseRegionServerPrincipal = ElementParameterParser.getValue(node, "__HBASE_REGIONSERVER_PRINCIPAL__");
%>
try{
	conn_<%=cid %> = org.apache.hadoop.hbase.HBaseConfiguration.create();
	conn_<%=cid %>.clear();
	conn_<%=cid %>.set("hbase.zookeeper.quorum", <%=zookeeper_quorum%>); 
	conn_<%=cid %>.set("hbase.zookeeper.property.clientPort",<%=zookeeper_client_port%>); 
	conn_<%=cid %>.set("hbase.cluster.distributed","true"); 
<%
	boolean setZNodeParent = "true".equals(ElementParameterParser.getValue(node, "__SET_ZNODE_PARENT__"));
	String zNodeParent = ElementParameterParser.getValue(node, "__ZNODE_PARENT__");		
	if(setZNodeParent) {
%>
	conn_<%=cid%>.set("zookeeper.znode.parent",<%=zNodeParent%>); 
<%
	}
	if((hbaseVersion!=null && supportKrbVersionList.contains(hbaseVersion)) && useKrb){
%>
		conn_<%=cid%>.set("hbase.master.kerberos.principal",<%=hbaseMasterPrincipal%>);
		conn_<%=cid%>.set("hbase.regionserver.kerberos.principal",<%=hbaseRegionServerPrincipal%>);
		conn_<%=cid%>.set("hbase.security.authorization","true");
		conn_<%=cid%>.set("hbase.security.authentication","kerberos");
<%
		if(useKeytab){
%>
			org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(<%=userPrincipal%>, <%=keytabPath%>);
<%
		}
	}

	List<Map<String, String>> properties =
        (List<Map<String,String>>)ElementParameterParser.getObjectValue(node,"__HBASE_PARAMETERS__");
   	for(int i=0;i<properties.size();i++){
   		Map<String, String> map = properties.get(i);
   		String property = map.get("PROPERTY");
   		String value= map.get("VALUE");
%>
		conn_<%=cid %>.set(<%=property%>,<%=value%>);
<%
   }
}else{// use existing connection
	String connection = ElementParameterParser.getValue(node,"__CONNECTION__");
	String conn = "conn_" + connection;
%>
	conn_<%=cid %> = (org.apache.hadoop.conf.Configuration)globalMap.get("<%=conn%>");
	if(conn_<%=cid %> == null){
		throw new RuntimeException("<%=cid %>'s connection is null!");
	}
<%
}

if(defineSelection) {
	String startRow = ElementParameterParser.getValue(node, "__START_ROW__");
	String endRow = ElementParameterParser.getValue(node, "__END_ROW__");
%>
	org.apache.hadoop.hbase.client.Scan scan_<%=cid %> = new org.apache.hadoop.hbase.client.Scan(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=startRow %>), org.apache.hadoop.hbase.util.Bytes.toBytes(<%=endRow %>));
<%
} else {
%>
	org.apache.hadoop.hbase.client.Scan scan_<%=cid %> = new org.apache.hadoop.hbase.client.Scan();
<%
}
List<IMetadataTable> metadatas = node.getMetadataList();
%>
<%
if ((metadatas!=null) && (metadatas.size() > 0)) {
    IMetadataTable metadata = metadatas.get(0);
    if (metadata != null) {
		List<IMetadataColumn> columns = metadata.getListColumns();
		List<Map<String, String>> mapping = (List<Map<String,String>>)ElementParameterParser.getObjectValue(node,"__MAPPING__");
		for(int i=0;i<mapping.size();i++){
			Map<String, String> map = mapping.get(i);
			IMetadataColumn column = columns.get(i);
			String schema_column = map.get("SCHEMA_COLUMN");
			String family_column= map.get("FAMILY_COLUMN");
%>
				scan_<%=cid %>.addColumn(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=family_column%>), org.apache.hadoop.hbase.util.Bytes.toBytes("<%=column.getOriginalDbColumnName()%>"));
<%
		}
		if(isByFilter){
			List<Map<String, String>> filterMapping = (List<Map<String,String>>)ElementParameterParser.getObjectValue(node,"__FILTER__");
			String logical = ElementParameterParser.getValue(node,"__LOGICAL_OP__");
			boolean hasMultipleColumnPrefixFilterType = false;
			for(int i=0;i<filterMapping.size();i++){
				Map<String, String> filterMap = filterMapping.get(i);
				String filterType = filterMap.get("FILTER_TYPE");
				if("MultipleColumnPrefixFilter".equals(filterType)){
					hasMultipleColumnPrefixFilterType = true;
					break;
				} 
			}
			if(hasMultipleColumnPrefixFilterType){
%>
				String [] multipleValues_<%=cid %> = null;
				byte [][] multipleBytesValues_<%=cid%> = null;
<%
			}
%>			
			org.apache.hadoop.hbase.filter.FilterList filterList_<%=cid%> = new org.apache.hadoop.hbase.filter.FilterList(org.apache.hadoop.hbase.filter.FilterList.Operator.<%=logical%>);
			org.apache.hadoop.hbase.filter.Filter filter_<%=cid%> = null;
<%		
			for(int j=0;j<filterMapping.size();j++){
				Map<String, String> filterMap = filterMapping.get(j);
				String filterType = filterMap.get("FILTER_TYPE");//"SingleColumnValueFilter","FamilyFilter","QualifierFilter","ColumnPrefixFilter","MultipleColumnPrefixFilter","MultipleColumnPrefixFilter","RowFilter"
				String filterColumn = filterMap.get("FILTER_COLUMN");
				String filterFamily = filterMap.get("FILTER_FAMILY");
				String filterOperation = filterMap.get("FILTER_OPERATOR");//"EQUAL","GREATER","GREATER_OR_EQUAL","LESS","LESS_OR_EQUAL","NO_OP","NOT_EQUAL",
				String filterValue = filterMap.get("FILTER_VALUE");
				String filterComparatorType = filterMap.get("FILTER_COMPARATOR_TYPE");//"BinaryComparator" ,"RegexStringComparator" ,"SubstringComparator"
				if("SingleColumnValueFilter".equals(filterType)){//"filterValue" is column value,like: "1" ,"2" ... ，return whole row (all columns) value 
					if("BinaryComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterFamily%>), org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterColumn%>), org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>, org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterValue%>));
<%
					}else if ("RegexStringComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterFamily%>), org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterColumn%>), org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>, new org.apache.hadoop.hbase.filter.RegexStringComparator(<%=filterValue%>));
<%
					}else if("SubstringComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.SingleColumnValueFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterFamily%>), org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterColumn%>), org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>, new org.apache.hadoop.hbase.filter.SubstringComparator(<%=filterValue%>));
<%
					}
				}else if("FamilyFilter".equals(filterType)){//"Filter Family" is family name ,like: "id_family","name_family"....， return columns which mapping in "Filter Family",filter other columns
					if("BinaryComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.FamilyFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterFamily%>)));
<%
					}else if ("RegexStringComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.FamilyFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.RegexStringComparator(<%=filterColumn%>));
<%
					}else if("SubstringComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.FamilyFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.SubstringComparator(<%=filterColumn%>));
<%
					}
				}else if("QualifierFilter".equals(filterType)){ //"Filter Column" is column name,like:"id" or "name" .... then you will get meet codition column value ,filter other columns
					if("BinaryComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterColumn%>)));
<%
					}else if ("RegexStringComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.RegexStringComparator(<%=filterColumn%>));
<%
					}else if("SubstringComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.QualifierFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.SubstringComparator(<%=filterColumn%>));
<%
					}
				}else if("ColumnPrefixFilter".equals(filterType)){//"Filter Column" value is column name,like:"id" or "name" ....,return column value,filter other columns
					if(filterFamily!=null && !"".equals(filterFamily)){ 
%>
						scan_<%=cid %>.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterFamily%>));
<%
					}
%>
					filter_<%=cid%> = new org.apache.hadoop.hbase.filter.ColumnPrefixFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterColumn%>));
<%
				}else if("MultipleColumnPrefixFilter".equals(filterType)){ //"Filter Column" value is for column name ,like:"id,name" ,"id,name,sex".... , return column value,filter other columns
					if(filterFamily!=null && !"".equals(filterFamily)){ 
%>
						scan_<%=cid %>.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterFamily%>));
<%
					}
%>
					if(<%=filterColumn%>!=null && !"".equals(<%=filterColumn%>)){
						multipleValues_<%=cid %> = <%=filterColumn%>.split(",");
						multipleBytesValues_<%=cid %> = new byte [multipleValues_<%=cid %>.length] [];
						for(int i=0;i< multipleValues_<%=cid %>.length;i++){
							multipleBytesValues_<%=cid %>[i] = org.apache.hadoop.hbase.util.Bytes.toBytes(multipleValues_<%=cid %>[i]);
						}
						filter_<%=cid%> =  new org.apache.hadoop.hbase.filter.MultipleColumnPrefixFilter(multipleBytesValues_<%=cid%>);
					}
<%
				}else if("ColumnRangeFilter".equals(filterType)){//"Filter Column" value is tow columns name,like: "id,name" ,"id,sex" ....,return columns value ,filter other columns
					if(filterFamily!=null && !"".equals(filterFamily)){ 
%>
						scan_<%=cid %>.addFamily(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterFamily%>));
<%
					}
%>
					filter_<%=cid%> = new org.apache.hadoop.hbase.filter.ColumnRangeFilter(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterColumn%>.split(",")[0]),true,org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterColumn%>.split(",")[1]),true);
<%
				}else if("RowFilter".equals(filterType)){//"Filter Value" is rowkey value,like "1" ,"car"....,return whole row (all columns)
					if("BinaryComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterValue%>)));
<%
					}else if ("RegexStringComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.RegexStringComparator(<%=filterValue%>));
<%
					}else if("SubstringComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.RowFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.SubstringComparator(<%=filterValue%>));
<%
					}
				}else if("ValueFilter".equals(filterType)){//"Filter Value" is any columns value,like "1" ,"car" .... ,return only the meet codition value,filter other columns
					if("BinaryComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.BinaryComparator(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=filterValue%>)));
<%
					}else if ("RegexStringComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.RegexStringComparator(<%=filterValue%>));
<%
					}else if("SubstringComparator".equals(filterComparatorType)){
%>
						filter_<%=cid%> = new org.apache.hadoop.hbase.filter.ValueFilter(org.apache.hadoop.hbase.filter.CompareFilter.CompareOp.<%=filterOperation%>,new org.apache.hadoop.hbase.filter.SubstringComparator(<%=filterValue%>));
<%
					}
				}
%>
					filterList_<%=cid%>.addFilter(filter_<%=cid%>);
<%
			}
%>
			scan_<%=cid %>.setFilter(filterList_<%=cid%>);
<%
		}
%>		
		org.apache.hadoop.hbase.client.HTable table_<%=cid %> = new org.apache.hadoop.hbase.client.HTable(conn_<%=cid %>, <%=table_name%>);
		String temp_<%=cid %>=null;
		byte[] rowResult_<%=cid %> = null;
		org.apache.hadoop.hbase.client.ResultScanner scanner_<%=cid %> = table_<%=cid %>.getScanner(scan_<%=cid %>);
<%

    	List< ? extends IConnection> conns = node.getOutgoingSortedConnections();
		if (conns != null){
			if (conns.size()>0){
				log4jFileUtil.startRetriveDataInfo();
%>
				for (org.apache.hadoop.hbase.client.Result rr_<%=cid %> = scanner_<%=cid %>.next(); rr_<%=cid %> != null; rr_<%=cid %> = scanner_<%=cid %>.next()) {
<%
					IConnection conn =conns.get(0);
					String connName = conn.getName();
					if (conn.getLineStyle().hasConnectionCategory(IConnectionCategory.DATA)) {
						for(int i=0;i<mapping.size();i++){
							Map<String, String> map = mapping.get(i);
							String schema_column = map.get("SCHEMA_COLUMN");
							String family_column= map.get("FAMILY_COLUMN");
							IMetadataColumn column = columns.get(i);
							String columnName = column.getLabel();
							String defaultValue = column.getDefault();
							if(columnName.equals(schema_column)) {//
								String typeToGenerate = JavaTypesManager.getTypeToGenerate(column.getTalendType(), column.isNullable());
								JavaType javaType = JavaTypesManager.getJavaTypeFromId(column.getTalendType());
								String patternValue = column.getPattern() == null || column.getPattern().trim().length() == 0 ? null : column.getPattern();
								boolean isPrimitiveType = JavaTypesManager.isJavaPrimitiveType(javaType, column.isNullable());
%>
								rowResult_<%=cid %> = rr_<%=cid %>.getValue(org.apache.hadoop.hbase.util.Bytes.toBytes(<%=family_column%>),org.apache.hadoop.hbase.util.Bytes.toBytes("<%=column.getOriginalDbColumnName()%>"));
								temp_<%=cid %> = org.apache.hadoop.hbase.util.Bytes.toString(rowResult_<%=cid %>);
								if(temp_<%=cid %>!=null && temp_<%=cid %>.length() > 0) {
<%									if(javaType == JavaTypesManager.STRING || javaType == JavaTypesManager.OBJECT) {
%>
										<%=connName %>.<%=columnName %>=temp_<%=cid %>.toString();
<%									}else if(javaType == JavaTypesManager.BYTE_ARRAY){
%> 
										<%=connName%>.<%=columnName%>=rowResult_<%=cid %>;
<%									}else if(javaType == JavaTypesManager.DATE){
%> 
										<%=connName%>.<%=columnName%>=ParserUtils.parseTo_Date(temp_<%=cid %>, <%= patternValue %>);
<%									}else if(isPrimitiveType && javaType == JavaTypesManager.INTEGER){  
%>
										<%=connName%>.<%=columnName%>=org.apache.hadoop.hbase.util.Bytes.toInt(rowResult_<%=cid %>);  
<%									}else if(isPrimitiveType && javaType == JavaTypesManager.CHARACTER){  
%>
										<%=connName%>.<%=columnName%>=(char)org.apache.hadoop.hbase.util.Bytes.toInt(rowResult_<%=cid %>);  
<%									}else if(isPrimitiveType && (javaType == JavaTypesManager.SHORT || javaType == JavaTypesManager.LONG || javaType == JavaTypesManager.FLOAT || javaType == JavaTypesManager.DOUBLE)) {  
%>
										<%=connName%>.<%=columnName%>=org.apache.hadoop.hbase.util.Bytes.to<%=javaType.getNullableClass().getSimpleName()%>(rowResult_<%=cid %>);  
<%									}else{
%>
										<%=connName%>.<%=columnName%>=ParserUtils.parseTo_<%= typeToGenerate %>(temp_<%=cid %>);
<%									}
%>
								}else{
<%
									String default_Value = JavaTypesManager.getDefaultValueFromJavaType(typeToGenerate, defaultValue);
									if(default_Value != null && !"null".equals(default_Value)) {
%>
										<%=connName %>.<%=columnName %> = <%=default_Value %>;
<%
									} else if(!JavaTypesManager.isJavaPrimitiveType(javaType,column.isNullable())) {
%>
										<%=connName %>.<%=columnName %> = null;
<%
									} else {
%>
										throw new RuntimeException("Value is empty for column : '<%=columnName %>' in '<%=connName %>' connection, value is invalid or this column should be nullable or have a default value.");									
<%
									}
%>
								}
<%
							} //if(columnName.equals(schema_column))
						} //for(int i=0;i<mapping.size();i++)
						
						log4jFileUtil.debugRetriveData(node,false);
						
					}//if(conn.getLineStyle().hasConnectionCategory(IConnectionCategory.DATA))
			}//if (conns.size()>0)
		}//if (conns != null)
	}//if (metadata != null)
}//if ((metadatas!=null) && (metadatas.size() > 0))
%>